"""
Adapter functions for testing train_grpo_ray_colocated.py components
Following the pattern established in adapters.py
"""

from typing import List, Dict, Any, Optional
import torch
import ray
from unittest.mock import Mock

# We'll import the actual classes when the dependencies are available
# For now, we'll create adapter functions that can be tested

def run_create_trajectory(
    prompts: List[str],
    responses: List[str], 
    rewards: torch.Tensor,
    log_probs: torch.Tensor,
    values: Optional[torch.Tensor] = None
) -> Dict[str, Any]:
    """
    Adapter for testing Trajectory creation and data access.
    Returns only numeric data that can be compared with numpy snapshots.
    
    Args:
        prompts: List of prompt strings
        responses: List of response strings  
        rewards: Tensor of rewards for each response
        log_probs: Tensor of log probabilities
        values: Optional tensor of value estimates
        
    Returns:
        Dict containing numeric trajectory data for testing
    """
    # Import here to avoid import errors in tests
    try:
        from cse599o_alignment.train_grpo_ray_colocated import Trajectory
        
        traj = Trajectory(
            prompts=prompts,
            responses=responses,
            rewards=rewards,
            log_probs=log_probs,
            values=values
        )
        
        # Return only numeric data for snapshot comparison
        result = {
            'rewards': traj.rewards,
            'log_probs': traj.log_probs,
            'num_prompts': len(traj.prompts),
            'num_responses': len(traj.responses),
            'rewards_shape': traj.rewards.shape,
            'log_probs_shape': traj.log_probs.shape
        }
        
        if traj.values is not None:
            result['values'] = traj.values
            result['values_shape'] = traj.values.shape
        
        return result
        
    except ImportError:
        # Return mock data for testing when imports fail
        result = {
            'rewards': rewards,
            'log_probs': log_probs,
            'num_prompts': len(prompts),
            'num_responses': len(responses), 
            'rewards_shape': rewards.shape,
            'log_probs_shape': log_probs.shape
        }
        
        if values is not None:
            result['values'] = values
            result['values_shape'] = values.shape
            
        return result


def run_generator_compute_advantages(
    trajectories_rewards: torch.Tensor,
    group_size: int = 4
) -> torch.Tensor:
    """
    Adapter for testing advantage computation logic.
    Implements a simple group-relative advantage computation for testing.
    
    Args:
        trajectories_rewards: Tensor of shape (num_trajectories * group_size,)
        group_size: Number of responses per group
        
    Returns:
        Tensor of advantages
    """
    # Reshape rewards into groups
    num_groups = trajectories_rewards.shape[0] // group_size
    grouped_rewards = trajectories_rewards.view(num_groups, group_size)
    
    # Compute group-relative advantages (reward - group_mean)
    group_means = grouped_rewards.mean(dim=1, keepdim=True)
    advantages = grouped_rewards - group_means
    
    return advantages.view(-1)


def run_compute_trajectory_statistics(
    trajectories_rewards: List[torch.Tensor]
) -> Dict[str, torch.Tensor]:
    """
    Adapter for testing trajectory statistics computation.
    
    Args:
        trajectories_rewards: List of reward tensors from trajectories
        
    Returns:
        Dict containing computed statistics
    """
    if not trajectories_rewards:
        return {
            'avg_reward': torch.tensor(0.0),
            'total_trajectories': torch.tensor(0),
            'reward_std': torch.tensor(0.0),
            'reward_min': torch.tensor(0.0),
            'reward_max': torch.tensor(0.0)
        }
    
    all_rewards = torch.cat(trajectories_rewards)
    
    return {
        'avg_reward': all_rewards.mean(),
        'total_trajectories': torch.tensor(len(trajectories_rewards)),
        'reward_std': all_rewards.std(),
        'reward_min': all_rewards.min(),
        'reward_max': all_rewards.max()
    }


def run_mock_training_step(
    prompts: List[str],
    step_count: int = 0
) -> Dict[str, Any]:
    """
    Adapter for testing training step logic without full implementation.
    
    Args:
        prompts: List of input prompts
        step_count: Current step count
        
    Returns:
        Dict containing training step results
    """
    # Mock trajectory generation
    num_trajectories = len(prompts)
    mock_rewards = torch.rand(num_trajectories * 4)  # G=4 responses per prompt
    mock_loss = torch.rand(1).item()
    
    return {
        'step': step_count + 1,
        'loss': mock_loss,
        'num_trajectories': num_trajectories,
        'avg_reward': float(mock_rewards.mean()),
        'prompts_processed': len(prompts)
    }


def run_validate_colocated_worker_config() -> Dict[str, Any]:
    """
    Adapter for testing ColocatedWorker configuration.
    
    Returns:
        Dict containing configuration validation results
    """
    try:
        from cse599o_alignment.train_grpo_ray_colocated import (
            G, VOCAB_SIZE, CONTEXT_LENGTH, NUM_LAYERS, D_MODEL
        )
        
        return {
            'group_size': G,
            'vocab_size': VOCAB_SIZE,
            'context_length': CONTEXT_LENGTH,
            'num_layers': NUM_LAYERS,
            'd_model': D_MODEL,
            'config_valid': True
        }
    except ImportError:
        return {
            'group_size': 4,
            'vocab_size': 50257,  # GPT-2 vocab size
            'context_length': 256,
            'num_layers': 4,
            'd_model': 512,
            'config_valid': False
        }


def run_test_ray_actor_creation(num_workers: int = 1) -> Dict[str, Any]:
    """
    Adapter for testing Ray actor creation and basic functionality.
    
    Args:
        num_workers: Number of workers to create
        
    Returns:
        Dict containing actor creation test results
    """
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True, _temp_dir="/local1/samarjit")
    
    try:
        from cse599o_alignment.train_grpo_ray_colocated import ColocatedWorker
        
        workers = []
        for i in range(num_workers):
            worker = ColocatedWorker.remote()
            workers.append(worker)
        
        # Test getting statistics from all workers
        stats_futures = [worker.get_statistics.remote() for worker in workers]
        stats_results = ray.get(stats_futures)
        
        # Cleanup
        for worker in workers:
            ray.kill(worker)
            
        return {
            'workers_created': num_workers,
            'workers_responsive': len(stats_results),
            'all_workers_initialized': all('step_count' in stats for stats in stats_results),
            'actor_creation_success': True
        }
        
    except Exception as e:
        return {
            'workers_created': 0,
            'workers_responsive': 0,
            'all_workers_initialized': False,
            'actor_creation_success': False,
            'error': str(e)
        }


def run_batch_prompts_processing(
    prompts: List[str],
    batch_size: int = 2
) -> Dict[str, Any]:
    """
    Adapter for testing batch processing of prompts.
    
    Args:
        prompts: List of prompts to process
        batch_size: Size of each batch
        
    Returns:
        Dict containing batch processing results
    """
    # Split prompts into batches
    batches = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i + batch_size]
        batches.append(batch)
    
    # Mock processing each batch
    batch_results = []
    for batch_idx, batch in enumerate(batches):
        batch_result = {
            'batch_id': batch_idx,
            'batch_size': len(batch),
            'prompts': batch,
            'mock_loss': torch.rand(1).item(),
            'mock_avg_reward': torch.rand(1).item()
        }
        batch_results.append(batch_result)
    
    return {
        'total_prompts': len(prompts),
        'num_batches': len(batches),
        'batch_results': batch_results,
        'avg_batch_size': sum(len(batch) for batch in batches) / len(batches),
        'processing_complete': True
    }
